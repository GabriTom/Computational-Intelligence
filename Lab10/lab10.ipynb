{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "Submission: Dies Natalis Solis Invicti\n",
    "Reviews: Befana\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "SOL_REF = [[4, 9, 2], [3, 5, 7], [8, 1, 6]]\n",
    "CMP_VALUE = 15\n",
    "N_ROUNDS_TRAIN = 2000\n",
    "N_ROUNDS_TEST = 10\n",
    "N_MATCHES = 10\n",
    "BASIC_VAL = 0\n",
    "CIRCLE = -1\n",
    "CROSS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class game, it has the actual state and the remaining moves\n",
    "class game:\n",
    "    def __init__(self):\n",
    "        self.state = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "        self.possible_moves = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that receives a state a computes if it is won or not\n",
    "def check_finished(state):\n",
    "    #Checking if cicle won thrpugh summing values and comparing it to the default value to check\n",
    "    circle_based_state = -np.multiply(state, SOL_REF)\n",
    "    win_circle_row = np.sum(circle_based_state, axis=0)\n",
    "    win_circle_col = np.sum(circle_based_state, axis=1)\n",
    "    win_circle_diag = circle_based_state[0, 0] + circle_based_state[1, 1] + circle_based_state[2, 2]\n",
    "    win_circle_anti_diag = circle_based_state[0, 2] + circle_based_state[1, 1] + circle_based_state[2, 0]\n",
    "    if CMP_VALUE in win_circle_row:\n",
    "        return CIRCLE\n",
    "    if CMP_VALUE in win_circle_col:\n",
    "        return CIRCLE\n",
    "    if CMP_VALUE == win_circle_diag:\n",
    "        return CIRCLE\n",
    "    if CMP_VALUE == win_circle_anti_diag:\n",
    "        return CIRCLE\n",
    "    #Same thing with cross\n",
    "    cross_based_state = np.multiply(state, SOL_REF)\n",
    "    win_cross_row = np.sum(cross_based_state, axis=0)\n",
    "    win_cross_col = np.sum(cross_based_state, axis=1)\n",
    "    win_cross_diag = cross_based_state[0, 0] + cross_based_state[1, 1] + cross_based_state[2, 2]\n",
    "    win_cross_anti_diag = cross_based_state[0, 2] + cross_based_state[1, 1] + cross_based_state[2, 0]\n",
    "    if CMP_VALUE in win_cross_row:\n",
    "        return CROSS\n",
    "    if CMP_VALUE in win_cross_col:\n",
    "        return CROSS\n",
    "    if CMP_VALUE == win_cross_diag:\n",
    "        return CROSS\n",
    "    if CMP_VALUE == win_cross_anti_diag:\n",
    "        return CROSS\n",
    "    \n",
    "    return BASIC_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class that identifies a player. Every player has a training_phase, but is only used if he is a Q-Learning Agent\n",
    "class player:\n",
    "    def __init__(self, sign):\n",
    "        self.sign = sign\n",
    "        self.exp_rate = 0.3\n",
    "        self.decay_gamma = 0.5\n",
    "        self.lr = 0.5\n",
    "        self.optimals = np.zeros((3, 3))\n",
    "        self.states = []\n",
    "        self.states_values = {}\n",
    "        self.training_phase = True\n",
    "\n",
    "    def feedReward(self, reward):\n",
    "        for st in reversed(self.states):\n",
    "            if self.states_values.get(st) is None:\n",
    "                self.states_values[st] = 0\n",
    "            self.states_values[st] += self.lr * (self.decay_gamma * reward - self.states_values[st])\n",
    "            reward = self.states_values[st]\n",
    "\n",
    "    def reset(self):\n",
    "        self.optimals = np.zeros((3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two strategies used during the masthes. The first one is completely random, while the second is an optimized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that receive a state and a player and plays a random move.\n",
    "def random_s(state, current_player):\n",
    "    row = random.randint(0, 2)\n",
    "    col = random.randint(0, 2)\n",
    "    while state[row][col] != BASIC_VAL:\n",
    "        row = random.randint(0, 2)\n",
    "        col = random.randint(0, 2)\n",
    "\n",
    "    state[row][col] = current_player.sign\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function that receives a state and a player and do the best move based on optimals values.\n",
    "def opt_s(state, current_player):\n",
    "\n",
    "    #Finding empty positions\n",
    "    available_positions = np.zeros((len(state), len(state[0])))\n",
    "    for i in range(len(state)):\n",
    "        for j in range(len(state[i])):\n",
    "            if state[i][j] == 0:\n",
    "                available_positions[i][j] = 1\n",
    "\n",
    "    compute_distance(state, current_player, available_positions)\n",
    "\n",
    "    #Selecting cell and performing the move\n",
    "    sr, sc = np.unravel_index(np.argmax(abs(current_player.optimals), axis=None), current_player.optimals.shape)\n",
    "    state[sr][sc] = current_player.sign\n",
    "    return state\n",
    "\n",
    "\n",
    "#Distance is computed and saved in players optimals\n",
    "def compute_distance(state, player, available_positions):\n",
    "    actual_optimals = player.optimals * available_positions\n",
    "    \n",
    "    sum_row = np.sum(state, axis=1).reshape(len(state[0]), 1)\n",
    "    for e in sum_row:\n",
    "        if e[0] == -2:\n",
    "            e[0] *= 2\n",
    "        #If the next move make you wins, choose that move\n",
    "        elif e == 2:\n",
    "            e[0] = e[0] * 2 + 1\n",
    "    actual_optimals += abs(sum_row)\n",
    "        \n",
    "    sum_col = np.sum(state, axis=0).reshape(1, len(state[1]))\n",
    "    for e in sum_col[0]:\n",
    "        if e == -2:\n",
    "            e *= 2\n",
    "        #If the next move make you wins, choose that move\n",
    "        elif e == 2:\n",
    "            e = e * 2 + 1\n",
    "    actual_optimals += abs(sum_col)\n",
    "\n",
    "    sum_diag = state[0][0] + state[1][1] + state[2][2]\n",
    "    if sum_diag == -2:\n",
    "        sum_diag *= 2\n",
    "    #If the next move make you wins, choose that move\n",
    "    elif sum_diag == 2:\n",
    "        sum_diag = sum_diag * 2 + 1\n",
    "    for i in range(player.optimals.shape[0]):\n",
    "        actual_optimals[i, i] += abs(sum_diag)\n",
    "\n",
    "    sum_anti_diag = state[0][2] + state[1][1] + state[2][0]\n",
    "    if sum_anti_diag == -2:\n",
    "        sum_anti_diag *= 2\n",
    "    #If the next move make you wins, choose that move\n",
    "    elif sum_anti_diag == 2:\n",
    "        sum_anti_diag = sum_anti_diag * 2 + 1\n",
    "    for i in range(player.optimals.shape[0]):\n",
    "        actual_optimals[player.optimals.shape[0]-1-i, i] += abs(sum_anti_diag)\n",
    "\n",
    "    player.optimals = np.multiply(actual_optimals, available_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a hash of a state (= state in a string)\n",
    "def get_hash(state):\n",
    "    ret = []\n",
    "    for row in state:\n",
    "        for cell in row:\n",
    "            ret.append(cell)\n",
    "    return str(ret)\n",
    "\n",
    "#Function that perform a move accordingly to the player exploration_rate and is the player is in traning phase or not\n",
    "def perform_move(state, player):\n",
    "    #Only player with cross is a Q-learned agent\n",
    "    if player.sign == CIRCLE:\n",
    "        move = random_s(state, player)\n",
    "    else:\n",
    "        available_positions_list = []\n",
    "        if np.random.uniform(0, 1) < player.exp_rate:\n",
    "            move = random_s(state, player)\n",
    "        else:\n",
    "            #If player is training itself perform the move following the optimal strategy, otherwise follows what it has learned\n",
    "            if player.training_phase:\n",
    "                move = opt_s(state, player)\n",
    "            else:\n",
    "                #Creating a list of available positions\n",
    "                for i in range(len(state)):\n",
    "                    for j in range(len(state[i])):\n",
    "                        if state[i][j] == 0:\n",
    "                            available_positions_list.append(i*len(state)+j)\n",
    "\n",
    "                value_max = -9999\n",
    "                for p in available_positions_list:\n",
    "                    next_state = deepcopy(state)\n",
    "                    row = p // len(state)\n",
    "                    col = p % len(state)\n",
    "                    next_state[row][col] = player.sign\n",
    "                    next_board_hash = get_hash(next_state)\n",
    "                    value = 0 if player.states_values.get(next_board_hash) is None else player.states_values.get(next_board_hash)\n",
    "                    # print(\"value\", value)\n",
    "                    if value >= value_max:\n",
    "                        value_max = value\n",
    "                        move = next_state\n",
    "\n",
    "        player.states.append(get_hash(move))\n",
    "\n",
    "    return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRAW!\n",
      "Player 2 wins most of the matches.\n",
      "[2, 7]\n"
     ]
    }
   ],
   "source": [
    "#Instantiating classes and play some games\n",
    "player_1 = player(CIRCLE)\n",
    "player_2 = player(CROSS)\n",
    "players = [player_1, player_2]\n",
    "\n",
    "#----------------- TRAINING PHASE -----------------\n",
    "for i in range(N_ROUNDS_TRAIN):\n",
    "    current_player = 0\n",
    "    match = game()\n",
    "    ending = check_finished(match.state)\n",
    "    \n",
    "    while ending == BASIC_VAL and match.possible_moves != 0:\n",
    "        match.state = perform_move(match.state, players[current_player])\n",
    "        match.possible_moves -= 1\n",
    "        current_player = abs(current_player - 1)\n",
    "        ending = check_finished(match.state)\n",
    "    \n",
    "    if ending == CROSS:\n",
    "        player_2.feedReward(1)\n",
    "        player_1.feedReward(0)\n",
    "    elif ending != BASIC_VAL:\n",
    "        player_2.feedReward(0)\n",
    "        player_1.feedReward(1)\n",
    "    else:\n",
    "        player_1.feedReward(0.5)\n",
    "        player_2.feedReward(0.1)\n",
    "\n",
    "    player_2.reset()\n",
    "\n",
    "#----------------- TESTING PHASE -----------------\n",
    "player_1.training_phase = False\n",
    "player_2.training_phase = False\n",
    "p_wins = [0, 0]\n",
    "p_matches = [0, 0]\n",
    "\n",
    "for i in range(N_ROUNDS_TEST):\n",
    "    current_player = 0\n",
    "    match = game()\n",
    "    ending = check_finished(match.state)\n",
    "    \n",
    "    while ending == BASIC_VAL and match.possible_moves > 0:\n",
    "        match.state = perform_move(match.state, players[current_player])\n",
    "        match.possible_moves -= 1\n",
    "        current_player = abs(current_player - 1)\n",
    "        ending = check_finished(match.state)\n",
    "\n",
    "    if ending == BASIC_VAL:\n",
    "        print(\"DRAW!\")\n",
    "    elif ending == CROSS:\n",
    "        p_wins[1] += 1\n",
    "    else:\n",
    "        p_wins[0] += 1\n",
    "\n",
    "if p_wins[0] > p_wins[1]:\n",
    "    print(\"Player 1 wins most of the matches.\")\n",
    "elif p_wins[0] == p_wins[1]:\n",
    "    print(\"Match draw\")\n",
    "else:\n",
    "    print(\"Player 2 wins most of the matches.\")\n",
    "print(p_wins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
